{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPVisionModel, CLIPVisionModelWithProjection\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.nn import DataParallel\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name).eval()\n",
    "clip_vision_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# clip_vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_res=clip_vision_model.vision_model.config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../data/mini_MathVista_grid/target/bar.png\"\n",
    "\n",
    "with Image.open(img_path) as img:\n",
    "    pil_image = img\n",
    "    img_tensor = torchvision.transforms.PILToTensor()(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = clip_vision_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=img_tensor, return_tensors=\"pt\")\n",
    "# processed_img = torch.from_numpy(np.array(processor.image_processor.preprocess(img_tensor)[\"pixel_values\"])).cuda()\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = clip_vision_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_tensor = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "pil_outputs = clip_vision_model(**pil_inputs)\n",
    "last_hidden_state_pil = pil_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_pil == last_hidden_state_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip import MyClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyClip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = model.encode_text(\"hello, my name e jeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEGE Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Adjust as needed\n",
    "\n",
    "from models.CLIP.clip import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "clip_rn_50,_ = clip.load('RN50', device=device)\n",
    "clip_rn_101,_ = clip.load('RN101', device=device)\n",
    "clip_vit_b_16,_ = clip.load('ViT-B/16', device=device)\n",
    "clip_vit_b_32,_ = clip.load('ViT-B/32', device=device)\n",
    "clip_vit_l_14,_ = clip.load('ViT-L/14', device=device)\n",
    "models = [clip_rn_50, clip_rn_101, clip_vit_b_16, clip_vit_b_32, clip_vit_l_14]\n",
    "# models = [clip_vit_b_32]\n",
    "clip_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "        torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "    ]\n",
    ")\n",
    "final_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(pic):\n",
    "    mode_to_nptype = {\"I\": np.int32, \"I;16\": np.int16, \"F\": np.float32}\n",
    "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
    "    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n",
    "    img = img.permute((2, 0, 1)).contiguous()\n",
    "    return img.to(dtype=torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "            torchvision.transforms.CenterCrop(224),\n",
    "            torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "            torchvision.transforms.Lambda(lambda img: to_tensor(img)),\n",
    "            torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        ]\n",
    "    )\n",
    "transform_fn_org = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(256, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "        torchvision.transforms.CenterCrop(256),\n",
    "        torchvision.transforms.ToTensor(), # [0, 1]\n",
    "        torchvision.transforms.Lambda(lambda img: (img * 2 - 1)),\n",
    "        # torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "        # torchvision.transforms.Lambda(lambda img: to_tensor(img)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_path = \"../data/mini_MathVista_grid/target/bar.png\"\n",
    "adv_path = \"../data/mini_MathVista_grid/target/abst.png\"\n",
    "\n",
    "with Image.open(tgt_path) as img:\n",
    "    pil_image = img\n",
    "    tgt_tensor = transform_fn(img.convert('RGB'))\n",
    "\n",
    "with Image.open(adv_path) as img:\n",
    "    pil_image = img\n",
    "    adv_tensor = transform_fn(img.convert('RGB'))\n",
    "    adv_tensor.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_tgt = img_tensor.to(torch.float16).to(device)\n",
    "image_tgt = tgt_tensor.to(device)\n",
    "image_adv = adv_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tgt = image_tgt.unsqueeze(0)\n",
    "image_adv = image_adv.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tgt_image_features_list=[]\n",
    "    image_tgt = clip_preprocess(image_tgt)\n",
    "    for clip_model in models:\n",
    "        tgt_image_features = clip_model.encode_image(image_tgt)  # [bs, 512]\n",
    "        tgt_image_features = tgt_image_features / tgt_image_features.norm(dim=1, keepdim=True)\n",
    "        tgt_image_features_list.append(tgt_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_image_features_list=[]\n",
    "image_adv = clip_preprocess(image_adv)\n",
    "for clip_model in models:\n",
    "    adv_image_features = clip_model.encode_image(image_adv)  # [bs, 512]\n",
    "    adv_image_features = adv_image_features / adv_image_features.norm(dim=1, keepdim=True)\n",
    "    adv_image_features_list.append(adv_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_losses=torch.zeros(len(models))\n",
    "loss = torch.zeros(1).to(device)\n",
    "crit_list = []\n",
    "for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "    model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.sum(torch.exp(tau*(costs[0] / costs[1]+1e-16)), dim=1) / (len(models)*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))\n",
    "\n",
    "model_losses=torch.zeros(len(models))\n",
    "loss = torch.zeros(1).to(device)\n",
    "for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "    model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "exp_cost_ratio = torch.exp(tau*(costs[1] / costs[0]+1e-16))\n",
    "weights = torch.sum(exp_cost_ratio, dim=0) / (len(models)*exp_cost_ratio)\n",
    "loss = torch.sum(weights * model_losses)\n",
    "\n",
    "costs[1] = costs[0]\n",
    "costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "# gradient = torch.clamp(gradient, min=-0.0025, max=0.0025)  # 0.0025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))\n",
    "for iteration in range(iterations):#pseudo code\n",
    "    model_losses=torch.zeros(len(models))\n",
    "    loss = torch.zeros(1).to(device)\n",
    "    crit_list = []\n",
    "    adv_image_features_list = get_adv_image_features_list(adv_image)#pseudo code\n",
    "    for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "        model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "    exp_cost_ratio = torch.exp(tau*(costs[1] / costs[0]+1e-16))\n",
    "    weights = torch.sum(exp_cost_ratio, dim=0) / (len(models)*exp_cost_ratio)\n",
    "    loss = torch.sum(weights * model_losses)\n",
    "\n",
    "    costs[1] = costs[0]\n",
    "    costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "    gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "    gradient = torch.clamp(gradient, min=-0.0025, max=0.0025)  # 0.0025\n",
    "\n",
    "    update_adv_image(adv_image) #pseudo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, feat in enumerate(tgt_image_features_list):\n",
    "#     if torch.all(torch.isnan(feat)):\n",
    "#         print(\"nothing here man\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Adjust as needed\n",
    "\n",
    "from models.CLIP.clip import clip\n",
    "\n",
    "# models = [clip_vit_b_32]\n",
    "clip_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "        torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "    ]\n",
    ")\n",
    "final_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class MyClipEnsemble():\n",
    "    def __init__(self, tau=2):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        clip_rn_50,_ = clip.load('RN50', device=device)\n",
    "        clip_rn_101,_ = clip.load('RN101', device=device)\n",
    "        clip_vit_b_16,_ = clip.load('ViT-B/16', device=device)\n",
    "        clip_vit_b_32,_ = clip.load('ViT-B/32', device=device)\n",
    "        clip_vit_l_14,_ = clip.load('ViT-L/14', device=device)\n",
    "        self.models = [clip_rn_50, clip_rn_101, clip_vit_b_16, clip_vit_b_32, clip_vit_l_14]\n",
    "\n",
    "        self.clip_preprocess = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "                # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "                torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "                torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.costs = torch.ones(2, len(self.models))\n",
    "        self.tau = tau\n",
    "        \n",
    "    def encode_image(self, image, use_grad=True):\n",
    "        image_features_list = []\n",
    "        image_tgt = clip_preprocess(image)\n",
    "\n",
    "        context = torch.enable_grad() if use_grad else torch.no_grad()\n",
    "        with context:\n",
    "            for clip_model in self.models:\n",
    "                image_features = clip_model.encode_image(image_tgt)  # [bs, 512]\n",
    "                image_features /= image_features.norm(dim=1, keepdim=True)\n",
    "                image_features_list.append(image_features)\n",
    "\n",
    "        return image_features_list\n",
    "    \n",
    "    def get_gradients(self, adv_image_features_list, tgt_image_features_list):\n",
    "        model_losses=torch.zeros(len(self.models))\n",
    "        loss = torch.zeros(1).to(self.device)\n",
    "        for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "            model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "        exp_cost_ratio = torch.exp(self.tau*(self.costs[1] / self.costs[0]+1e-16))\n",
    "        weights = torch.sum(exp_cost_ratio, dim=0) / (len(self.models)*exp_cost_ratio)\n",
    "        loss = torch.sum(weights * model_losses)\n",
    "\n",
    "        self.costs[1] = self.costs[0]\n",
    "        self.costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "        gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))\n",
    "\n",
    "model_losses=torch.zeros(len(models))\n",
    "loss = torch.zeros(1).to(device)\n",
    "for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "    model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "exp_cost_ratio = torch.exp(tau*(costs[1] / costs[0]+1e-16))\n",
    "weights = torch.sum(exp_cost_ratio, dim=0) / (len(models)*exp_cost_ratio)\n",
    "loss = torch.sum(weights * model_losses)\n",
    "\n",
    "costs[1] = costs[0]\n",
    "costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "gradient = torch.autograd.grad(loss, adv_tensor)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
