{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mxy/miniconda3/envs/VLM_Poisoning/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/mxy/miniconda3/envs/VLM_Poisoning/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2025-01-09 20:24:18,253] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPVisionModel, CLIPVisionModelWithProjection\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.nn import DataParallel\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name).eval()\n",
    "clip_vision_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# clip_vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_res=clip_vision_model.vision_model.config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../data/mini_MathVista_grid/target/bar.png\"\n",
    "\n",
    "with Image.open(img_path) as img:\n",
    "    pil_image = img\n",
    "    img_tensor = torchvision.transforms.PILToTensor()(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 553, 699])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = clip_vision_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=img_tensor, return_tensors=\"pt\")\n",
    "# processed_img = torch.from_numpy(np.array(processor.image_processor.preprocess(img_tensor)[\"pixel_values\"])).cuda()\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = clip_vision_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_tensor = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=699x553>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "pil_outputs = clip_vision_model(**pil_inputs)\n",
    "last_hidden_state_pil = pil_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state_pil == last_hidden_state_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-09 23:15:27,154] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from clip import MyClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyClip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = model.encode_text(\"hello, my name e jeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEGE Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Adjust as needed\n",
    "\n",
    "from models.CLIP.clip import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "clip_rn_50,_ = clip.load('RN50', device=device)\n",
    "clip_rn_101,_ = clip.load('RN101', device=device)\n",
    "clip_vit_b_16,_ = clip.load('ViT-B/16', device=device)\n",
    "clip_vit_b_32,_ = clip.load('ViT-B/32', device=device)\n",
    "clip_vit_l_14,_ = clip.load('ViT-L/14', device=device)\n",
    "models = [clip_rn_50, clip_rn_101, clip_vit_b_16, clip_vit_b_32, clip_vit_l_14]\n",
    "# models = [clip_vit_b_32]\n",
    "clip_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "        torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "    ]\n",
    ")\n",
    "final_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(pic):\n",
    "    mode_to_nptype = {\"I\": np.int32, \"I;16\": np.int16, \"F\": np.float32}\n",
    "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
    "    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n",
    "    img = img.permute((2, 0, 1)).contiguous()\n",
    "    return img.to(dtype=torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "            torchvision.transforms.CenterCrop(224),\n",
    "            torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "            torchvision.transforms.Lambda(lambda img: to_tensor(img)),\n",
    "            torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        ]\n",
    "    )\n",
    "transform_fn_org = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(256, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "        torchvision.transforms.CenterCrop(256),\n",
    "        torchvision.transforms.ToTensor(), # [0, 1]\n",
    "        torchvision.transforms.Lambda(lambda img: (img * 2 - 1)),\n",
    "        # torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "        # torchvision.transforms.Lambda(lambda img: to_tensor(img)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_path = \"../data/mini_MathVista_grid/target/bar.png\"\n",
    "adv_path = \"../data/mini_MathVista_grid/target/abst.png\"\n",
    "\n",
    "with Image.open(tgt_path) as img:\n",
    "    pil_image = img\n",
    "    tgt_tensor = transform_fn(img.convert('RGB'))\n",
    "\n",
    "with Image.open(adv_path) as img:\n",
    "    pil_image = img\n",
    "    adv_tensor = transform_fn(img.convert('RGB'))\n",
    "    adv_tensor.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_tgt = img_tensor.to(torch.float16).to(device)\n",
    "image_tgt = tgt_tensor.to(device)\n",
    "image_adv = adv_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tgt = image_tgt.unsqueeze(0)\n",
    "image_adv = image_adv.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tgt_image_features_list=[]\n",
    "    image_tgt = clip_preprocess(image_tgt)\n",
    "    for clip_model in models:\n",
    "        tgt_image_features = clip_model.encode_image(image_tgt)  # [bs, 512]\n",
    "        tgt_image_features = tgt_image_features / tgt_image_features.norm(dim=1, keepdim=True)\n",
    "        tgt_image_features_list.append(tgt_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_image_features_list=[]\n",
    "image_adv = clip_preprocess(image_adv)\n",
    "for clip_model in models:\n",
    "    adv_image_features = clip_model.encode_image(image_adv)  # [bs, 512]\n",
    "    adv_image_features = adv_image_features / adv_image_features.norm(dim=1, keepdim=True)\n",
    "    adv_image_features_list.append(adv_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_losses=torch.zeros(len(models))\n",
    "loss = torch.zeros(1).to(device)\n",
    "crit_list = []\n",
    "for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "    model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "torch.sum(torch.exp(tau*(costs[0] / costs[1]+1e-16)), dim=1) / (len(models)*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m costs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m costs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m costs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m model_losses\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 17\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# gradient = torch.clamp(gradient, min=-0.0025, max=0.0025)  # 0.0025\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_Poisoning/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))\n",
    "\n",
    "model_losses=torch.zeros(len(models))\n",
    "loss = torch.zeros(1).to(device)\n",
    "for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "    model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "exp_cost_ratio = torch.exp(tau*(costs[1] / costs[0]+1e-16))\n",
    "weights = torch.sum(exp_cost_ratio, dim=0) / (len(models)*exp_cost_ratio)\n",
    "loss = torch.sum(weights * model_losses)\n",
    "\n",
    "costs[1] = costs[0]\n",
    "costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "# gradient = torch.clamp(gradient, min=-0.0025, max=0.0025)  # 0.0025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0025, -0.0025, -0.0025,  ..., -0.0025, -0.0019,  0.0025],\n",
       "         [-0.0025, -0.0025, -0.0025,  ...,  0.0025,  0.0025,  0.0025],\n",
       "         [-0.0025,  0.0025, -0.0025,  ...,  0.0025,  0.0025,  0.0025],\n",
       "         ...,\n",
       "         [-0.0025, -0.0025, -0.0025,  ...,  0.0025, -0.0025,  0.0025],\n",
       "         [-0.0025,  0.0025, -0.0025,  ...,  0.0025,  0.0011,  0.0025],\n",
       "         [ 0.0012,  0.0025, -0.0025,  ...,  0.0025,  0.0025,  0.0025]],\n",
       "\n",
       "        [[-0.0025,  0.0025, -0.0025,  ..., -0.0025,  0.0025,  0.0025],\n",
       "         [ 0.0025,  0.0025,  0.0025,  ..., -0.0025, -0.0025,  0.0025],\n",
       "         [ 0.0025,  0.0025,  0.0025,  ..., -0.0025, -0.0025,  0.0025],\n",
       "         ...,\n",
       "         [-0.0025, -0.0025, -0.0025,  ..., -0.0025, -0.0025,  0.0025],\n",
       "         [-0.0025, -0.0025, -0.0025,  ..., -0.0025, -0.0025,  0.0025],\n",
       "         [ 0.0025, -0.0025, -0.0025,  ...,  0.0025,  0.0025,  0.0025]],\n",
       "\n",
       "        [[-0.0025, -0.0025, -0.0025,  ...,  0.0025,  0.0025,  0.0025],\n",
       "         [-0.0025,  0.0025, -0.0025,  ..., -0.0025,  0.0025,  0.0025],\n",
       "         [-0.0025, -0.0025,  0.0016,  ..., -0.0025,  0.0025,  0.0025],\n",
       "         ...,\n",
       "         [ 0.0025,  0.0025, -0.0025,  ..., -0.0025, -0.0025,  0.0025],\n",
       "         [ 0.0025,  0.0022,  0.0003,  ..., -0.0025, -0.0025,  0.0025],\n",
       "         [ 0.0025, -0.0025, -0.0025,  ...,  0.0025,  0.0025,  0.0025]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))\n",
    "for iteration in range(iterations):#pseudo code\n",
    "    model_losses=torch.zeros(len(models))\n",
    "    loss = torch.zeros(1).to(device)\n",
    "    crit_list = []\n",
    "    adv_image_features_list = get_adv_image_features_list(adv_image)#pseudo code\n",
    "    for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "        model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "    exp_cost_ratio = torch.exp(tau*(costs[1] / costs[0]+1e-16))\n",
    "    weights = torch.sum(exp_cost_ratio, dim=0) / (len(models)*exp_cost_ratio)\n",
    "    loss = torch.sum(weights * model_losses)\n",
    "\n",
    "    costs[1] = costs[0]\n",
    "    costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "    gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "    gradient = torch.clamp(gradient, min=-0.0025, max=0.0025)  # 0.0025\n",
    "\n",
    "    update_adv_image(adv_image) #pseudo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.3464, 1.2880, 1.3026,  ..., 1.3172, 1.3026, 1.3026],\n",
       "          [1.3610, 1.3026, 1.3172,  ..., 1.3172, 1.3026, 1.3026],\n",
       "          [1.3756, 1.3172, 1.3464,  ..., 1.3172, 1.3026, 1.3026],\n",
       "          ...,\n",
       "          [1.3026, 1.2880, 1.3026,  ..., 1.3318, 1.3318, 1.3318],\n",
       "          [1.3318, 1.3026, 1.3026,  ..., 1.3318, 1.3318, 1.3318],\n",
       "          [1.3756, 1.2880, 1.3026,  ..., 1.3318, 1.3318, 1.3318]],\n",
       "\n",
       "         [[1.3095, 1.2645, 1.2795,  ..., 1.3095, 1.2945, 1.2945],\n",
       "          [1.3245, 1.2795, 1.2945,  ..., 1.3095, 1.2945, 1.2945],\n",
       "          [1.3395, 1.2945, 1.3095,  ..., 1.3095, 1.2945, 1.2945],\n",
       "          ...,\n",
       "          [1.3695, 1.3545, 1.3545,  ..., 1.3545, 1.3545, 1.3545],\n",
       "          [1.3695, 1.3545, 1.3545,  ..., 1.3545, 1.3545, 1.3545],\n",
       "          [1.3995, 1.3395, 1.3545,  ..., 1.3545, 1.3545, 1.3545]],\n",
       "\n",
       "         [[1.1932, 1.0794, 1.0936,  ..., 1.0510, 1.0367, 1.0225],\n",
       "          [1.1932, 1.0794, 1.1078,  ..., 1.0510, 1.0367, 1.0225],\n",
       "          [1.1932, 1.0794, 1.1221,  ..., 1.0510, 1.0367, 1.0225],\n",
       "          ...,\n",
       "          [1.2216, 1.1363, 1.1363,  ..., 1.1078, 1.1078, 1.1078],\n",
       "          [1.2216, 1.1505, 1.1363,  ..., 1.1078, 1.1078, 1.1078],\n",
       "          [1.2643, 1.1505, 1.1363,  ..., 1.1078, 1.1078, 1.1078]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, feat in enumerate(tgt_image_features_list):\n",
    "#     if torch.all(torch.isnan(feat)):\n",
    "#         print(\"nothing here man\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing here man 0\n",
      "nothing here man 1\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Adjust as needed\n",
    "\n",
    "from models.CLIP.clip import clip\n",
    "\n",
    "# models = [clip_vit_b_32]\n",
    "clip_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "        torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "    ]\n",
    ")\n",
    "final_preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "        # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "        torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class MyClipEnsemble():\n",
    "    def __init__(self, tau=2):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        clip_rn_50,_ = clip.load('RN50', device=device)\n",
    "        clip_rn_101,_ = clip.load('RN101', device=device)\n",
    "        clip_vit_b_16,_ = clip.load('ViT-B/16', device=device)\n",
    "        clip_vit_b_32,_ = clip.load('ViT-B/32', device=device)\n",
    "        clip_vit_l_14,_ = clip.load('ViT-L/14', device=device)\n",
    "        self.models = [clip_rn_50, clip_rn_101, clip_vit_b_16, clip_vit_b_32, clip_vit_l_14]\n",
    "\n",
    "        self.clip_preprocess = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(clip_vit_b_32.visual.input_resolution, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "                # torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "                torchvision.transforms.CenterCrop(clip_vit_b_32.visual.input_resolution),\n",
    "                torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.costs = torch.ones(2, len(self.models))\n",
    "        self.tau = tau\n",
    "        \n",
    "    def encode_image(self, image, use_grad=True):\n",
    "        image_features_list = []\n",
    "        image_tgt = clip_preprocess(image)\n",
    "\n",
    "        context = torch.enable_grad() if use_grad else torch.no_grad()\n",
    "        with context:\n",
    "            for clip_model in self.models:\n",
    "                image_features = clip_model.encode_image(image_tgt)  # [bs, 512]\n",
    "                image_features /= image_features.norm(dim=1, keepdim=True)\n",
    "                image_features_list.append(image_features)\n",
    "\n",
    "        return image_features_list\n",
    "    \n",
    "    def get_gradients(self, adv_image_features_list, tgt_image_features_list):\n",
    "        model_losses=torch.zeros(len(self.models))\n",
    "        loss = torch.zeros(1).to(self.device)\n",
    "        for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "            model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "        exp_cost_ratio = torch.exp(self.tau*(self.costs[1] / self.costs[0]+1e-16))\n",
    "        weights = torch.sum(exp_cost_ratio, dim=0) / (len(self.models)*exp_cost_ratio)\n",
    "        loss = torch.sum(weights * model_losses)\n",
    "\n",
    "        self.costs[1] = self.costs[0]\n",
    "        self.costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "        gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0  \n",
    "costs = torch.ones(2, len(models))\n",
    "\n",
    "model_losses=torch.zeros(len(models))\n",
    "loss = torch.zeros(1).to(device)\n",
    "for model_i, (pred_i, target_i) in enumerate(zip(adv_image_features_list, tgt_image_features_list)):\n",
    "    model_losses[model_i] = torch.mean(torch.sum(pred_i * target_i, dim=1))   \n",
    "\n",
    "exp_cost_ratio = torch.exp(tau*(costs[1] / costs[0]+1e-16))\n",
    "weights = torch.sum(exp_cost_ratio, dim=0) / (len(models)*exp_cost_ratio)\n",
    "loss = torch.sum(weights * model_losses)\n",
    "\n",
    "costs[1] = costs[0]\n",
    "costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "gradient = torch.autograd.grad(loss, adv_tensor)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
