{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-06 15:14:23,491] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from parallelformers import parallelize\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.nn import DataParallel\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import numpy as np\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "d_type=torch.float16\n",
    "\n",
    "def get_image_encoder_internlm():\n",
    "    vision_tower_name=\"internlm/internlm-xcomposer2d5-clip\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    vision_tower = CLIPVisionModel.from_pretrained(vision_tower_name).eval().cuda()\n",
    "    vision_tower.half()\n",
    "    vision_tower.requires_grad_(True)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        vision_tower = DataParallel(vision_tower)\n",
    "\n",
    "    img_size=560\n",
    "\n",
    "    return vision_tower, None, None, img_size\n",
    "\n",
    "def encode_image_internlm(image_encoder, X_adv, img_size, bs, diff_aug, orig_sizes):\n",
    "    images = []\n",
    "    for j in range(bs):\n",
    "        orig_w, orig_h = orig_sizes[j]\n",
    "\n",
    "        img = X_adv[j][:, :orig_h, :orig_w]\n",
    "        img = __resize_img__(img)\n",
    "        img = torch.nn.functional.interpolate(img.unsqueeze(0), size=(img_size, img_size), mode='bicubic')\n",
    "\n",
    "        if diff_aug:\n",
    "            img = diff_aug(img).cuda()\n",
    "        else:\n",
    "            img = img.cuda()\n",
    "\n",
    "\n",
    "        images.append(img)\n",
    "\n",
    "    images = torch.cat(images, dim=0).cuda()\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=d_type):\n",
    "        image_embeds = image_encoder(images)\n",
    "\n",
    "    return image_embeds.last_hidden_state\n",
    "\n",
    "def __resize_img__(img):\n",
    "    \"\"\"Resize the image with padding to maintain aspect ratio.\"\"\"\n",
    "    _, h, w = img.shape\n",
    "    target_size = max(h, w)\n",
    "    \n",
    "    # Calculate padding\n",
    "    top_padding = (target_size - h) // 2\n",
    "    bottom_padding = target_size - h - top_padding\n",
    "    left_padding = (target_size - w) // 2\n",
    "    right_padding = target_size - w - left_padding\n",
    "\n",
    "    # Apply padding to make the image square\n",
    "    padded_img = torchvision.transforms.functional.pad(\n",
    "        img, [left_padding, top_padding, right_padding, bottom_padding]\n",
    "    )\n",
    "\n",
    "    return padded_img\n",
    "\n",
    "def get_internlm_model():\n",
    "    model_name=\"internlm/internlm-xcomposer2d5-7b\"\n",
    "    d_type=torch.float16\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # self.accelerator = Accelerator()\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,  # Use 8-bit quantization\n",
    "        llm_int8_threshold=200.0  # Adjust threshold for 8-bit quantization if necessary\n",
    "    )\n",
    "\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name, \n",
    "        torch_dtype=d_type, \n",
    "        # quantization_config=quantization_config,\n",
    "        trust_remote_code=True, \n",
    "        device_map='auto'\n",
    "    )\n",
    "    # self.model = self.accelerator.prepare(self.model)\n",
    "    # parallelize(self.model, num_gpus=2, fp16=True, verbose='detail')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model.tokenizer = tokenizer\n",
    "    num_beams = 3\n",
    "    return model, tokenizer\n",
    "\n",
    "def Image_transform(img, hd_num=25):\n",
    "    width, height = img.size\n",
    "    trans = False\n",
    "    if width < height:\n",
    "        img = img.transpose(Image.TRANSPOSE)\n",
    "        trans = True\n",
    "        width, height = img.size\n",
    "    ratio = (width/ height)\n",
    "    scale = 1\n",
    "    while scale*np.ceil(scale/ratio) <= hd_num:\n",
    "        scale += 1\n",
    "    scale -= 1\n",
    "    scale = min(np.ceil(width / 560), scale)\n",
    "    new_w = int(scale * 560)\n",
    "    new_h = int(new_w / ratio)\n",
    "    #print (scale, f'{height}/{new_h}, {width}/{new_w}')\n",
    "\n",
    "    img = transforms.functional.resize(img, [new_h, new_w],)\n",
    "    img = padding_336(img, 560)\n",
    "    width, height = img.size\n",
    "    if trans:\n",
    "        img = img.transpose(Image.TRANSPOSE)\n",
    "\n",
    "    return img\n",
    "\n",
    "def padding_336(b, pad=336):\n",
    "    width, height = b.size\n",
    "    tar = int(np.ceil(height / pad) * pad)\n",
    "    top_padding = 0 # int((tar - height)/2)\n",
    "    bottom_padding = tar - height - top_padding\n",
    "    left_padding = 0\n",
    "    right_padding = 0\n",
    "    b = transforms.functional.pad(b, [left_padding, top_padding, right_padding, bottom_padding], fill=[255,255,255])\n",
    "\n",
    "    return b\n",
    "\n",
    "def get_response_internlm(image, text_prompt, tokenizer, model):\n",
    "        query = text_prompt\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                print(transforms.ToTensor()(Image_transform(image)).shape)\n",
    "                response, his = model.chat(tokenizer, query, transforms.ToTensor()(Image_transform(image)).unsqueeze(0).unsqueeze(0), do_sample=False, num_beams=3, use_meta=True)\n",
    "        prediction = response.strip()\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set max length to 16384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c846b67bd461fb5bfef8aeb665ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_internlm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = lambda image: get_response_internlm(transforms.ToPILImage()(image), \"what is in this image\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transforms.ToTensor()(Image.open(\"../data/task_data/Mini_MathVista_base_hamburgerFries_target/target_train/0.png\").convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1120, 1120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image is a compilation of various data visualization charts and graphs, each representing a different type of data representation. It includes a Bar Graph, a Pie Chart, a Line Graph, a Scatter Plot, a Histogram, a Network Graph, a Heat Map, and a Box Plot. These visualizations are commonly used in the field of data science and analytics to represent data in a way that is easy to understand and analyze.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_image_files(directory):\n",
    "        \"\"\"\n",
    "        Lists all JPG and PNG files in a given directory.\n",
    "\n",
    "        Args:\n",
    "            directory (str): Path to the directory.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of image file paths.\n",
    "        \"\"\"\n",
    "        image_files = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    image_files.append(os.path.join(root, file))\n",
    "        return image_files\n",
    "\n",
    "# Get image files from both directories\n",
    "image_files_dir1 = list_image_files(\"../data/poisons/llava/Mini_MathVista_base_hamburgerFries_target/image\")\n",
    "image_files_dir2 = list_image_files(\"../data/task_data/Mini_MathVista_base_hamburgerFries_target/base_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number(file_path):\n",
    "    match = re.search(r'(\\d+)', file_path)  # Extract the number using regex\n",
    "    return int(match.group(1)) if match else float('inf')  # Return the number or inf if no number is found\n",
    "\n",
    "\n",
    "image_files_dir1 = sorted(image_files_dir1, key=extract_number)\n",
    "image_files_dir2 = sorted(image_files_dir2, key=extract_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "i = 0\n",
    "for poison, base in zip(image_files_dir1, image_files_dir2):\n",
    "    img1 = Image.open(poison)\n",
    "    img2 = Image.open(base)\n",
    "\n",
    "    # Set a fixed height for the images and adjust their aspect ratio\n",
    "    fixed_height = 600\n",
    "    img1_resized = img1.resize((int(img1.width * fixed_height / img1.height), fixed_height))\n",
    "    img2_resized = img2.resize((int(img2.width * fixed_height / img2.height), fixed_height))\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Adjust figsize as needed\n",
    "\n",
    "    # Display the first image\n",
    "    axes[0].imshow(img1_resized)\n",
    "    axes[0].axis(\"off\")  # Turn off axes for a cleaner look\n",
    "\n",
    "    # Display the second image\n",
    "    axes[1].imshow(img2_resized)\n",
    "    axes[1].axis(\"off\")  # Turn off axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    i+=1\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
