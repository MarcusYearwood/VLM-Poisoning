{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mxy/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 12:16:11,651] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mxy/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d461e06d13934c2ea801c6e9e4c561fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bcd748106c413c9b43f77ae2ddda5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, CLIPImageProcessor\n",
    "from torch import nn\n",
    "\n",
    "models = [\n",
    "    AutoModel.from_pretrained(\n",
    "    'OpenGVLab/InternViT-300M-448px-V2_5',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True).cuda().eval(),\n",
    "    AutoModel.from_pretrained(\n",
    "    'OpenGVLab/InternViT-6B-448px-V2_5',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True).cuda().eval(),\n",
    "    AutoModel.from_pretrained(\n",
    "    'OpenGVLab/InternViT-6B-448px-V1-5',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True).cuda().eval()\n",
    "]\n",
    "\n",
    "image_processors = [\n",
    "    CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-300M-448px-V2_5'), \n",
    "    CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-6B-448px-V2_5'), \n",
    "    CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-6B-448px-V1-5'), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_path = \"../data/mini_MathVista_grid/target/bar.png\"\n",
    "# adv_path = \"../data/poisons/mini_MathVista_grid+i2i_InternEnsembleAttack/bar/31.png\"\n",
    "base_path = \"../data/mini_MathVista_grid/base_512/31.jpg\"\n",
    "adv_path = base_path\n",
    "\n",
    "tgt_image = Image.open(tgt_path).convert('RGB')\n",
    "adv_image = Image.open(adv_path).convert('RGB')\n",
    "\n",
    "adv_pixel_values = [image_processor(images=adv_image, return_tensors='pt').pixel_values for image_processor in image_processors]\n",
    "tgt_pixel_values = [image_processor(images=tgt_image, return_tensors='pt').pixel_values for image_processor in image_processors]\n",
    "\n",
    "adv_pixel_values = [adv_pixel_value.to(torch.bfloat16).cuda() for adv_pixel_value in adv_pixel_values]\n",
    "tgt_pixel_values = [tgt_pixel_value.to(torch.bfloat16).cuda() for tgt_pixel_value in tgt_pixel_values]\n",
    "\n",
    "with torch.no_grad():\n",
    "    adv_outputs = [model(adv_pixel_value) for model, adv_pixel_value in zip(models, adv_pixel_values)]\n",
    "    tgt_outputs = [model(tgt_pixel_value) for model, tgt_pixel_value in zip(models, tgt_pixel_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0859, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(0.4766, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(0.4082, device='cuda:0', dtype=torch.bfloat16)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "similarity = [cos(adv_output.last_hidden_state.view(-1), tgt_output.last_hidden_state.view(-1)) for adv_output, tgt_output in zip(adv_outputs, tgt_outputs)]\n",
    "similarity # .9922 (adv)\n",
    "\n",
    "# [tensor(0.7188, device='cuda:0', dtype=torch.bfloat16),\n",
    "#  tensor(0.9102, device='cuda:0', dtype=torch.bfloat16),\n",
    "#  tensor(0.6602, device='cuda:0', dtype=torch.bfloat16)] adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base: \n",
    "[tensor(0.2246, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward1>),\n",
    " tensor(0.5430, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward1>)]\n",
    "\n",
    "adv: \n",
    "[tensor(0.2178, device='cuda:0', dtype=torch.bfloat16),\n",
    " tensor(0.5586, device='cuda:0', dtype=torch.bfloat16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1396, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.2178, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(0.5586, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(0.4707, device='cuda:0', dtype=torch.bfloat16)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.sum(adv_output.last_hidden_state/torch.norm(adv_output.last_hidden_state)*tgt_output.last_hidden_state/torch.norm(tgt_output.last_hidden_state)) for adv_output, tgt_output in zip(adv_outputs, tgt_outputs)] # 0.2178 (adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1025, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInternEnsemble():\n",
    "    def __init__(self, tau=2):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = [\n",
    "            AutoModel.from_pretrained(\n",
    "            'OpenGVLab/InternViT-300M-448px-V2_5',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True).to(self.device).eval(),\n",
    "\n",
    "            AutoModel.from_pretrained(\n",
    "            'OpenGVLab/InternViT-6B-448px-V2_5',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True).to(self.device).eval(),\n",
    "\n",
    "            AutoModel.from_pretrained(\n",
    "            'OpenGVLab/InternViT-6B-448px-V1-5',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True).to(self.device).eval()\n",
    "        ]\n",
    "\n",
    "        self.preprocess = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(448, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "                torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0) / 255.0),\n",
    "                torchvision.transforms.CenterCrop(448),\n",
    "                torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)), # CLIP imgs mean and std.\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.costs = torch.ones(2, len(self.models)).to(self.device)\n",
    "        self.tau = tau\n",
    "        self.critical = nn.CosineSimilarity(dim=0, eps=1e-6).to(self.device)\n",
    "\n",
    "\n",
    "        \n",
    "    def encode_image(self, image, use_grad=True):\n",
    "        image_features_list = []\n",
    "        image_tgt = self.preprocess(image)\n",
    "        assert len(image_tgt.size()) == 3\n",
    "\n",
    "        context = torch.enable_grad() if use_grad else torch.no_grad()\n",
    "        with context:\n",
    "            for model in self.models:\n",
    "                image_features = model(image_tgt.unsqueeze(0)).last_hidden_state  # [bs, 512]\n",
    "                image_features = image_features / image_features.norm()\n",
    "                image_features_list.append(image_features)\n",
    "\n",
    "        return image_features_list\n",
    "    \n",
    "    def get_gradients(self, adv_image_features_list, tgt_image_features_list, adv_tensor):\n",
    "        model_losses=torch.zeros(len(self.models))\n",
    "        loss = torch.zeros(1).to(self.device)\n",
    "        model_losses = torch.stack([self.critical(adv_embed.view(-1), tgt_embed.view(-1)) for adv_embed, tgt_embed in zip(adv_image_features_list, tgt_image_features_list)])\n",
    "\n",
    "        exp_cost_ratio = torch.exp(self.tau*(self.costs[1] / self.costs[0]+1e-16))\n",
    "        weights = torch.sum(exp_cost_ratio, dim=0) / (len(self.models)*exp_cost_ratio)\n",
    "        loss = torch.sum(weights * model_losses)\n",
    "\n",
    "        self.costs[1] = self.costs[0]\n",
    "        self.costs[0] = model_losses.clone().detach()\n",
    "\n",
    "\n",
    "        gradient = torch.autograd.grad(loss, adv_tensor)[0]\n",
    "        return gradient, torch.mean(model_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 07:09:41,147] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285811aa596b4473808e3378a91f2b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65e8cc4ca52426297ccab06d6076431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "ensemble = MyInternEnsemble()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5.2929e-05,  5.0545e-05,  5.8174e-05,  ..., -3.9816e-05,\n",
       "           -2.7657e-05,  2.4438e-05],\n",
       "          [-4.8876e-05,  2.2531e-05,  5.6505e-05,  ..., -9.0599e-06,\n",
       "           -4.6968e-05, -1.3411e-05],\n",
       "          [-4.2200e-05, -6.9141e-06,  1.6451e-05,  ...,  2.8491e-05,\n",
       "           -2.8849e-05, -8.3447e-05],\n",
       "          ...,\n",
       "          [-7.5817e-05, -4.1723e-05, -5.3883e-05,  ...,  9.0003e-06,\n",
       "            2.1744e-04,  1.6308e-04],\n",
       "          [-8.1539e-05, -4.4346e-05, -3.6716e-05,  ...,  2.6822e-05,\n",
       "            2.1458e-04,  2.0599e-04],\n",
       "          [-8.0585e-05, -3.7193e-05, -1.8358e-05,  ...,  1.4782e-05,\n",
       "            4.2915e-06, -2.8729e-05]],\n",
       " \n",
       "         [[-4.6492e-05, -7.5698e-06,  6.1691e-06,  ..., -2.6822e-05,\n",
       "            3.7104e-06, -3.4094e-05],\n",
       "          [-3.8385e-05, -2.2650e-06, -5.5730e-06,  ..., -2.0027e-05,\n",
       "            3.0398e-05, -5.1498e-05],\n",
       "          [-3.6240e-05, -1.2338e-05,  1.0133e-05,  ...,  3.2663e-05,\n",
       "            1.1861e-05, -2.4676e-05],\n",
       "          ...,\n",
       "          [-4.0531e-05, -4.1962e-05, -5.8115e-06,  ...,  7.7724e-05,\n",
       "            1.8883e-04,  1.4114e-04],\n",
       "          [-5.4359e-05, -9.5963e-06, -1.9550e-05,  ...,  1.3924e-04,\n",
       "            1.4591e-04,  1.1587e-04],\n",
       "          [-3.7670e-05,  7.9274e-06,  5.5134e-06,  ..., -4.2200e-05,\n",
       "           -1.1921e-04, -1.7738e-04]],\n",
       " \n",
       "         [[-2.6822e-05,  6.9141e-06, -1.8239e-05,  ..., -3.2663e-05,\n",
       "            1.3709e-05, -2.0266e-05],\n",
       "          [-2.9802e-05, -1.0014e-05, -2.8014e-05,  ..., -1.4544e-05,\n",
       "           -3.5018e-06, -8.6308e-05],\n",
       "          [-1.5259e-05, -1.1146e-05,  3.3617e-05,  ..., -1.5497e-05,\n",
       "           -1.2457e-05, -1.1110e-04],\n",
       "          ...,\n",
       "          [-6.8247e-06, -3.2410e-07, -2.0027e-05,  ...,  1.0538e-04,\n",
       "            1.5831e-04,  5.5313e-05],\n",
       "          [ 2.1219e-05,  1.6451e-05, -1.1325e-05,  ...,  5.6982e-05,\n",
       "            1.7166e-04,  7.8201e-05],\n",
       "          [ 2.0981e-05,  5.1022e-05,  1.4126e-05,  ..., -4.6730e-05,\n",
       "           -1.0777e-04, -1.7929e-04]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(0.4180, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_tensor(pic):\n",
    "    mode_to_nptype = {\"I\": np.int32, \"I;16\": np.int16, \"F\": np.float32}\n",
    "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
    "    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n",
    "    img = img.permute((2, 0, 1)).contiguous()\n",
    "    return img.to(dtype=torch.get_default_dtype())\n",
    "\n",
    "transform_fn = torchvision.transforms.Compose(\n",
    "    [\n",
    "        # torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "        # torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "        torchvision.transforms.Lambda(lambda img: to_tensor(img)),\n",
    "        torchvision.transforms.Lambda(lambda img: torch.clamp(img, 0.0, 255.0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tgt_path = \"../data/mini_MathVista_grid/target/bar.png\"\n",
    "adv_path = \"../data/poisons/mini_MathVista_grid+i2i_EnsembleAttack/bar/1.png\"\n",
    "# base_path = \"../data/mini_MathVista_grid/base_512/1.jpg\"\n",
    "# adv_path = base_path\n",
    "\n",
    "tgt_image = Image.open(tgt_path).convert('RGB')\n",
    "adv_image = Image.open(adv_path).convert('RGB')\n",
    "\n",
    "\n",
    "\n",
    "adv_pixel_values = transform_fn(adv_image).to(torch.bfloat16).cuda().requires_grad_()\n",
    "tgt_pixel_values = transform_fn(tgt_image).to(torch.bfloat16).cuda()\n",
    "\n",
    "adv_image_features_list = ensemble.encode_image(adv_pixel_values)\n",
    "tgt_image_features_list = ensemble.encode_image(tgt_pixel_values, use_grad=False)\n",
    "\n",
    "ensemble.get_gradients(adv_image_features_list, tgt_image_features_list, adv_pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLM_Poisoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
