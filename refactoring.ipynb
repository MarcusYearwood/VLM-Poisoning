{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-07 07:06:44,639] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "from training_models import MyClipEnsemble\n",
    "\n",
    "from datasets import collate_fn_image, ImageDataset, EnsembleImageDataset\n",
    "from poison_utils import L2_norm\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mini_MathVista_grid\"\n",
    "method = \"i2i_EnsembleAttack\"\n",
    "args = {\n",
    "    \"model\": \"clip\",\n",
    "    \"poison_save_pth\": f\"./data/poisons/{dataset_name}+{method}\",\n",
    "    \"iter_attack\": 100,\n",
    "    \"lr_attack\": 0.5,\n",
    "    \"base_data_pth\": f\"data/{dataset_name}/base\",\n",
    "    \"target_data_pth\": f\"data/{dataset_name}/target\",\n",
    "    \"questions_pth\": f\"data/{dataset_name}/questions.json\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"eps\": 8,\n",
    "}\n",
    "def read_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# encoder_model = MyClip()\n",
    "encoder_model = MyClipEnsemble()\n",
    "device = encoder_model.device\n",
    "\n",
    "question_json = read_json(args[\"questions_pth\"])\n",
    "pids = question_json.keys()\n",
    "\n",
    "\n",
    "\n",
    "base_caps = read_json(os.path.join(args[\"base_data_pth\"], \"caps.json\"))\n",
    "target_caps = read_json(os.path.join(args[\"target_data_pth\"], \"caps.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dataset = ImageDataset(base_caps[\"annotations\"])\n",
    "# target_dataset = ImageDataset(target_caps[\"annotations\"])\n",
    "base_dataset = EnsembleImageDataset(base_caps[\"annotations\"])\n",
    "target_dataset = EnsembleImageDataset(target_caps[\"annotations\"])\n",
    "\n",
    "base_dataloader = torch.utils.data.DataLoader(\n",
    "    base_dataset, batch_size=1, \n",
    "    shuffle=False, collate_fn=collate_fn_image\n",
    ")\n",
    "\n",
    "target_dataloader = torch.utils.data.DataLoader(\n",
    "    target_dataset, batch_size=1, \n",
    "    shuffle=False, collate_fn=collate_fn_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_img(img, caption):\n",
    "    print(caption)\n",
    "    plt.imshow(img.squeeze(0).permute(1,2,0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2i_attack(model, image_base, image_target, iters=100, lr=1/255, eps=8):\n",
    "      '''\n",
    "      optimizing x_adv to minimize emb_dist( img_embed of x_adv, img_embed of image_target ) within Lp constraint using PGD\n",
    "\n",
    "      model: model class with image embedding functionality (e.g. CLIP, EVA)\n",
    "      image_base, image_target: images between [0,1] \n",
    "      emb_dist: the distance metrics for vision embedding (such as L2): take a batch of bs image pairs as input, \\\n",
    "            and output EACH of pair-wise distances of the whole batch (size = [bs])\n",
    "\n",
    "      eps: for Lp constraint\n",
    "      lr: the step size. The update is grad.sign * lr\n",
    "      diff_aug: using differentiable augmentation, e.g. RandomResizeCrop\n",
    "\n",
    "      return: X_adv between [0,1]\n",
    "      '''\n",
    "\n",
    "      bs = image_base.size(0)\n",
    "      device = image_base.device\n",
    "\n",
    "      with torch.no_grad():\n",
    "            embedding_targets = model.encode_image(image_target)\n",
    "            embedding_targets = embedding_targets / embedding_targets.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      X_adv = image_base.clone().detach()\n",
    "      X_adv.requires_grad_(True) \n",
    "\n",
    "      loss_best = 1e8\n",
    "      X_adv_best = X_adv.clone().detach()\n",
    "\n",
    "      # for i in tqdm(range(iters), leave=False):\n",
    "      for i in range(iters):\n",
    "            embs = model.encode_image(X_adv)\n",
    "            embs = embs / embs.norm(dim=-1, keepdim=True)  # Normalize embeddings\n",
    "\n",
    "            loss = L2_norm(embs, embedding_targets)\n",
    "            # loss = torch.mean(torch.sum(embs * embedding_targets, dim=1))\n",
    "\n",
    "            if loss < loss_best:\n",
    "                  loss_best = loss.clone().detach()\n",
    "                  X_adv_best = X_adv.clone().detach()\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # print losses at increments\n",
    "            if i% max(int(iters/20),1) == 0:\n",
    "                  # print('Iter :{} loss:{:.4f}, lr * 255:{:.4f}'.format(i,loss.item()/bs, scheduler.get_last_lr()[0]*255))\n",
    "                  print('Iter :{} loss:{:.4f}, lr * 255:{:.4f}'.format(i,loss.item()/bs, lr*255))\n",
    "\n",
    "            grad = X_adv.grad\n",
    "            assert grad != None\n",
    "            grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)           \n",
    "            \n",
    "            perturbation = lr * grad.sign()\n",
    "            # print(perturbation.shape)\n",
    "            X_adv.data = X_adv.data.detach() + perturbation\n",
    "            X_adv.data = torch.minimum(torch.maximum(X_adv, image_base - eps), image_base + eps) \n",
    "            X_adv.data = X_adv.data.clamp(0,255)\n",
    "            X_adv.grad = None  \n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                  print('Encounter nan loss at iteration {}'.format(i))\n",
    "                  break                 \n",
    "\n",
    "      print('Best Total loss:{:.4f}'.format(loss.item()))\n",
    "\n",
    "      return X_adv_best, loss_best\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGAImageAttack(model, image, cap, iters=100, lr=1/255, eps=8):\n",
    "      '''\n",
    "      optimizing x_adv to maximize emb_dist( img_embed of x_adv, text_embed of captions ) within Lp constraint using PGD\n",
    "\n",
    "      model: model class with image and text embedding functionality (e.g. CLIP, EVA)\n",
    "      image, cap: image between [0,255] (float) caption dict for that image\n",
    "      emb_dist: the distance metrics for vision embedding (such as L2): take a batch of bs image pairs as input, \\\n",
    "            and output EACH of pair-wise distances of the whole batch (size = [bs])\n",
    "\n",
    "      eps: for Lp constraint\n",
    "      lr: the step size. The update is grad.sign * lr\n",
    "\n",
    "      return: X_adv between [0,255]\n",
    "      '''\n",
    "\n",
    "      bs = image.size(0)\n",
    "      device = image.device\n",
    "\n",
    "      with torch.no_grad():\n",
    "            embedding_targets = model.encode_text(cap['caption'])\n",
    "            embedding_targets = embedding_targets / embedding_targets.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      X_adv = image.clone().detach()\n",
    "      X_adv.requires_grad_(True) \n",
    "\n",
    "      loss_best = 1e8\n",
    "      X_adv_best = X_adv.clone().detach()\n",
    "\n",
    "      # for i in tqdm(range(iters), leave=False):\n",
    "      for i in range(iters):\n",
    "            embs = model.encode_image(X_adv)\n",
    "            embs = embs / embs.norm(dim=-1, keepdim=True)  # Normalize embeddings\n",
    "\n",
    "            loss = -L2_norm(embs, embedding_targets)\n",
    "\n",
    "            if loss < loss_best:\n",
    "                  loss_best = loss.clone().detach()\n",
    "                  X_adv_best = X_adv.clone().detach()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # print losses at increments\n",
    "            if i% max(int(iters/20),1) == 0:\n",
    "                  print('Iter :{} loss:{:.4f}, lr * 255:{:.4f}'.format(i,loss.item()/bs, lr*255))\n",
    "\n",
    "            grad = X_adv.grad\n",
    "            assert grad != None\n",
    "            grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)           \n",
    "            \n",
    "            perturbation = lr * grad.sign()\n",
    "            # print(perturbation.shape)\n",
    "            X_adv.data = X_adv.data.detach() - perturbation\n",
    "            X_adv.data = torch.minimum(torch.maximum(X_adv, image_base - eps), image_base + eps) \n",
    "            X_adv.data = X_adv.data.clamp(0,255)\n",
    "            X_adv.grad = None  \n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                  print('Encounter nan loss at iteration {}'.format(i))\n",
    "                  break                 \n",
    "\n",
    "      print('Best Total loss:{:.4f}'.format(loss.item()))\n",
    "\n",
    "      return X_adv_best, loss_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2i_EnsembleAttack(model, image, target, iters=100, lr=1/255, eps=8):\n",
    "      '''\n",
    "      optimizing x_adv to maximize emb_dist( img_embed of x_adv, text_embed of captions ) within Lp constraint using PGD\n",
    "\n",
    "      model: model class with image and text embedding functionality (e.g. CLIP, EVA)\n",
    "      image, cap: image between [0,255] (float) caption dict for that image\n",
    "      emb_dist: the distance metrics for vision embedding (such as L2): take a batch of bs image pairs as input, \\\n",
    "            and output EACH of pair-wise distances of the whole batch (size = [bs])\n",
    "\n",
    "      eps: for Lp constraint\n",
    "      lr: the step size. The update is grad.sign * lr\n",
    "\n",
    "      return: X_adv between [0,255]\n",
    "      '''\n",
    "\n",
    "      bs = image.size(0)\n",
    "      device = image.device\n",
    "\n",
    "      with torch.no_grad():\n",
    "            embedding_targets = model.encode_image(target)\n",
    "\n",
    "      X_adv = image.clone().detach()\n",
    "      X_adv.requires_grad_(True) \n",
    "\n",
    "      loss_best = -1e8\n",
    "      X_adv_best = X_adv.clone().detach()\n",
    "\n",
    "      # for i in tqdm(range(iters), leave=False):\n",
    "      for i in range(iters):\n",
    "            embs = model.encode_image(X_adv, use_grad=True)\n",
    "\n",
    "            # loss = L2_norm(embs, embedding_targets)\n",
    "            grad, loss = model.get_gradients(embs, embedding_targets, X_adv) \n",
    "\n",
    "            if loss > loss_best:\n",
    "                  loss_best = loss.clone().detach()\n",
    "                  X_adv_best = X_adv.clone().detach()\n",
    "\n",
    "            # print losses at increments\n",
    "            if i% max(int(iters/20),1) == 0:\n",
    "                  print('Iter :{} loss:{:.4f}, lr:{:.4f}'.format(i,loss.item()/bs, lr))\n",
    "\n",
    "            \n",
    "            perturbation = lr * grad.sign() / 255.0\n",
    "            # print(perturbation.shape)\n",
    "            with torch.no_grad():\n",
    "                  X_adv = X_adv + perturbation\n",
    "                  X_adv = torch.min(torch.max(X_adv, image_base - eps/255.0), image_base + eps/255.0)\n",
    "                  X_adv = torch.clamp(X_adv, 0, 1)\n",
    "            X_adv.requires_grad_(True)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                  print('Encounter nan loss at iteration {}'.format(i))\n",
    "                  break                 \n",
    "\n",
    "      print('Best Total loss:{:.4f}'.format(loss.item()))\n",
    "\n",
    "      # return X_adv.clone().detach(), loss_best\n",
    "      return X_adv_best, loss_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss gets logits and maximizes entropy of each generated word for image caption\n",
    "\n",
    "# def MaximizeInformationEntropyAttack(model, image, cap, iters=100, lr=1/255, eps=8):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGA Image Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# ###### Running SGA Image Attack ######\n",
    "# original_sizes_list = []\n",
    "# X_adv_list = []\n",
    "# all_losses = []\n",
    "# loss_attack_list = []\n",
    "\n",
    "# target_path = os.path.join(args['poison_save_pth'], \"SGA\")\n",
    "\n",
    "# if not os.path.exists(target_path):\n",
    "#     os.makedirs(target_path)\n",
    "# saved_images = set(os.listdir(target_path))\n",
    "# saved_image_ids = {int(fname.split('.')[0]) for fname in saved_images if fname.endswith(('.png', '.jpg', '.jpeg'))}\n",
    "\n",
    "\n",
    "# for i, (image_base, base_cap) in  tqdm(enumerate(base_dataloader), desc=f\"Processing all images\", total=len(base_dataloader)):\n",
    "#     if base_cap['name'] in saved_image_ids:\n",
    "#             print(f\"{base_cap['name']} already processed for {target_cap['name']}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#     print('image name = ', base_cap['name'])\n",
    "#     image_base = image_base.to(device)\n",
    "\n",
    "#     X_adv, loss_attack = SGAImageAttack(\n",
    "#             model=encoder_model,\n",
    "#             image=image_base,\n",
    "#             cap=base_cap,\n",
    "#             iters=args['iter_attack'],\n",
    "#             lr=args['lr_attack'],\n",
    "#             eps=args['eps']\n",
    "#     )\n",
    "\n",
    "\n",
    "#     ###### Save poisoned images after each batch ######\n",
    "#     img_pth = os.path.join(target_path, f\"{base_cap['name']}.png\")\n",
    "#     image_to_save = X_adv/255.0\n",
    "#     print()\n",
    "#     if base_cap['name'] not in saved_image_ids:  # Only save if it doesn't already exist\n",
    "#         print(\"Max Pixel Difference between Adversarial Image and Base *255:\", round(torch.max(torch.abs(X_adv-image_base)).item(), 4))\n",
    "#         save_image(image_to_save.cpu(), img_pth)\n",
    "#         print(f\"Saved poisoned image {base_cap['name']} to {img_pth}\") # mathvista\n",
    "\n",
    "#     # show_img(((X_adv-image_base).to(torch.uint8).cpu()*10), (\"Adversarial Noise\", img_pth))\n",
    "#     # show_img(image_to_save.cpu(), (\"Adversarial Image\", img_pth))\n",
    "\n",
    "        \n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i2i attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Running i2i attack ######\n",
    "# original_sizes_list = []\n",
    "# X_adv_list = []\n",
    "# all_losses = []\n",
    "# loss_attack_list = []\n",
    "# start_idx = 0\n",
    "# end_idx = 0\n",
    "# # for j, (image_target, target_cap) in  tqdm(enumerate(target_dataloader)):\n",
    "# for j, (image_target, target_cap) in  tqdm(enumerate(target_dataloader), desc=\"Target Dataloader\", position=0, leave=True):\n",
    "#     target_name = target_cap[\"name\"]\n",
    "#     target_path = os.path.join(args[\"poison_save_pth\"], target_cap[\"name\"])\n",
    "#     if not os.path.exists(target_path):\n",
    "#         os.makedirs(target_path)\n",
    "\n",
    "#     ###### Resume by checking already saved images ######\n",
    "#     saved_images = set(os.listdir(target_path))\n",
    "#     saved_image_ids = {int(fname.split('.')[0]) for fname in saved_images if fname.endswith(('.png', '.jpg', '.jpeg'))}\n",
    "\n",
    "#     # show_img(image_target, (\"Working on target:\", target_name))\n",
    "\n",
    "#     for i, (image_base, base_cap) in  tqdm(enumerate(base_dataloader), desc=f\"Processing {target_name}\", total=len(base_dataloader), position=1, leave=False):\n",
    "#         if base_cap['name'] in saved_image_ids:\n",
    "#                 print(f\"{base_cap['name']} already processed for {target_cap['name']}, skipping...\")\n",
    "#                 continue\n",
    "\n",
    "#         print('image name = ', base_cap['name'])\n",
    "#         image_base, image_target = image_base.to(device), image_target.to(device)\n",
    "\n",
    "#         X_adv, loss_attack = i2i_attack(\n",
    "#                 model=encoder_model,\n",
    "#                 image_base=image_base,\n",
    "#                 image_target=image_target,\n",
    "#                 iters=args['iter_attack'],\n",
    "#                 lr=args['lr_attack'],\n",
    "#                 eps=args['eps']\n",
    "#         )\n",
    "\n",
    "\n",
    "#         ###### Save poisoned images after each batch ######\n",
    "#         img_pth = os.path.join(target_path, f\"{base_cap['name']}.png\")\n",
    "#         image_to_save = X_adv/255.0\n",
    "#         print()\n",
    "#         if base_cap['name'] not in saved_image_ids:  # Only save if it doesn't already exist\n",
    "#             print(\"Max Pixel Difference between Adversarial Image and Base *255:\", round(torch.max(torch.abs(X_adv-image_base)).item(), 4))\n",
    "#             save_image(image_to_save.cpu(), img_pth)\n",
    "#             print(f\"Saved poisoned image {base_cap['name']} to {img_pth}\") # mathvista\n",
    "\n",
    "#         # show_img(((X_adv-image_base).to(torch.uint8).cpu()*10), (\"Adversarial Noise\", img_pth))\n",
    "#         # show_img(image_to_save.cpu(), (\"Adversarial Image\", img_pth))\n",
    "\n",
    "        \n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i2i Ensemble attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Dataloader: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image name =  85\n",
      "Iter :0 loss:0.4771, lr:0.5000\n",
      "Iter :5 loss:0.6007, lr:0.5000\n",
      "Iter :10 loss:0.6810, lr:0.5000\n",
      "Iter :15 loss:0.7259, lr:0.5000\n",
      "Iter :20 loss:0.7715, lr:0.5000\n",
      "Iter :25 loss:0.7914, lr:0.5000\n",
      "Iter :30 loss:0.8071, lr:0.5000\n",
      "Iter :35 loss:0.8280, lr:0.5000\n",
      "Iter :40 loss:0.8331, lr:0.5000\n",
      "Iter :45 loss:0.8428, lr:0.5000\n",
      "Iter :50 loss:0.8529, lr:0.5000\n",
      "Iter :55 loss:0.8580, lr:0.5000\n",
      "Iter :60 loss:0.8668, lr:0.5000\n",
      "Iter :65 loss:0.8723, lr:0.5000\n",
      "Iter :70 loss:0.8705, lr:0.5000\n",
      "Iter :75 loss:0.8700, lr:0.5000\n",
      "Iter :80 loss:0.8803, lr:0.5000\n",
      "Iter :85 loss:0.8779, lr:0.5000\n",
      "Iter :90 loss:0.8851, lr:0.5000\n",
      "Iter :95 loss:0.8897, lr:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Total loss:0.8904\n",
      "\n",
      "Max Pixel Difference between Adversarial Image and Base *255: 8.00000712275505\n",
      "Saved poisoned image 85 to ./data/poisons/mini_MathVista_grid+i2i_EnsembleAttack/bar/85.png\n",
      "image name =  54\n",
      "Iter :0 loss:0.5550, lr:0.5000\n",
      "Iter :5 loss:0.5976, lr:0.5000\n",
      "Iter :10 loss:0.6423, lr:0.5000\n",
      "Iter :15 loss:0.6705, lr:0.5000\n",
      "Iter :20 loss:0.6905, lr:0.5000\n",
      "Iter :25 loss:0.7063, lr:0.5000\n",
      "Iter :30 loss:0.7240, lr:0.5000\n",
      "Iter :35 loss:0.7280, lr:0.5000\n",
      "Iter :40 loss:0.7399, lr:0.5000\n",
      "Iter :45 loss:0.7490, lr:0.5000\n",
      "Iter :50 loss:0.7591, lr:0.5000\n",
      "Iter :55 loss:0.7684, lr:0.5000\n",
      "Iter :60 loss:0.7769, lr:0.5000\n",
      "Iter :65 loss:0.7748, lr:0.5000\n",
      "Iter :70 loss:0.7802, lr:0.5000\n",
      "Iter :75 loss:0.7788, lr:0.5000\n",
      "Iter :80 loss:0.7824, lr:0.5000\n",
      "Iter :85 loss:0.7900, lr:0.5000\n",
      "Iter :90 loss:0.7931, lr:0.5000\n",
      "Iter :95 loss:0.7971, lr:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Total loss:0.7998\n",
      "\n",
      "Max Pixel Difference between Adversarial Image and Base *255: 8.00000712275505\n",
      "Saved poisoned image 54 to ./data/poisons/mini_MathVista_grid+i2i_EnsembleAttack/bar/54.png\n",
      "image name =  28\n",
      "Iter :0 loss:0.5532, lr:0.5000\n",
      "Iter :5 loss:0.5857, lr:0.5000\n",
      "Iter :10 loss:0.6195, lr:0.5000\n",
      "Iter :15 loss:0.6400, lr:0.5000\n",
      "Iter :20 loss:0.6523, lr:0.5000\n",
      "Iter :25 loss:0.6661, lr:0.5000\n",
      "Iter :30 loss:0.6760, lr:0.5000\n",
      "Iter :35 loss:0.6923, lr:0.5000\n",
      "Iter :40 loss:0.6990, lr:0.5000\n",
      "Iter :45 loss:0.7090, lr:0.5000\n",
      "Iter :50 loss:0.7182, lr:0.5000\n",
      "Iter :55 loss:0.7209, lr:0.5000\n",
      "Iter :60 loss:0.7247, lr:0.5000\n",
      "Iter :65 loss:0.7249, lr:0.5000\n",
      "Iter :70 loss:0.7361, lr:0.5000\n",
      "Iter :75 loss:0.7364, lr:0.5000\n",
      "Iter :80 loss:0.7432, lr:0.5000\n",
      "Iter :85 loss:0.7449, lr:0.5000\n",
      "Iter :90 loss:0.7441, lr:0.5000\n",
      "Iter :95 loss:0.7417, lr:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Total loss:0.7537\n",
      "\n",
      "Max Pixel Difference between Adversarial Image and Base *255: 8.00000712275505\n",
      "Saved poisoned image 28 to ./data/poisons/mini_MathVista_grid+i2i_EnsembleAttack/bar/28.png\n",
      "image name =  82\n",
      "Iter :0 loss:0.4966, lr:0.5000\n",
      "Iter :5 loss:0.5589, lr:0.5000\n",
      "Iter :10 loss:0.5917, lr:0.5000\n",
      "Iter :15 loss:0.6045, lr:0.5000\n",
      "Iter :20 loss:0.6194, lr:0.5000\n",
      "Iter :25 loss:0.6345, lr:0.5000\n",
      "Iter :30 loss:0.6481, lr:0.5000\n",
      "Iter :35 loss:0.6541, lr:0.5000\n",
      "Iter :40 loss:0.6651, lr:0.5000\n",
      "Iter :45 loss:0.6718, lr:0.5000\n",
      "Iter :50 loss:0.6808, lr:0.5000\n",
      "Iter :55 loss:0.6904, lr:0.5000\n",
      "Iter :60 loss:0.6844, lr:0.5000\n",
      "Iter :65 loss:0.7012, lr:0.5000\n",
      "Iter :70 loss:0.6940, lr:0.5000\n",
      "Iter :75 loss:0.7125, lr:0.5000\n",
      "Iter :80 loss:0.7121, lr:0.5000\n",
      "Iter :85 loss:0.7181, lr:0.5000\n",
      "Iter :90 loss:0.7202, lr:0.5000\n",
      "Iter :95 loss:0.7242, lr:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Total loss:0.7253\n",
      "\n",
      "Max Pixel Difference between Adversarial Image and Base *255: 8.00000712275505\n",
      "Saved poisoned image 82 to ./data/poisons/mini_MathVista_grid+i2i_EnsembleAttack/bar/82.png\n",
      "image name =  53\n",
      "Iter :0 loss:0.3440, lr:0.5000\n",
      "Iter :5 loss:0.4472, lr:0.5000\n",
      "Iter :10 loss:0.5204, lr:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Dataloader: 0it [00:56, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter :15 loss:0.5458, lr:0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage name = \u001b[39m\u001b[38;5;124m'\u001b[39m, base_cap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m image_base, image_target \u001b[38;5;241m=\u001b[39m image_base\u001b[38;5;241m.\u001b[39mto(device), image_target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m X_adv, loss_attack \u001b[38;5;241m=\u001b[39m \u001b[43mi2i_EnsembleAttack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miter_attack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_attack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m###### Save poisoned images after each batch ######\u001b[39;00m\n\u001b[1;32m     40\u001b[0m img_pth \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(target_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_cap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mi2i_EnsembleAttack\u001b[0;34m(model, image, target, iters, lr, eps)\u001b[0m\n\u001b[1;32m     30\u001b[0m embs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(X_adv, use_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# loss = L2_norm(embs, embedding_targets)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m grad, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_adv\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m>\u001b[39m loss_best:\n\u001b[1;32m     36\u001b[0m       loss_best \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/project/xtmp/mxy/VLM-Poisoning/training_models/clip.py:161\u001b[0m, in \u001b[0;36mMyClipEnsemble.get_gradients\u001b[0;34m(self, adv_image_features_list, tgt_image_features_list, adv_tensor)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosts[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosts[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m model_losses\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 161\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient, torch\u001b[38;5;241m.\u001b[39mmean(model_losses)\n",
      "File \u001b[0;32m~/miniconda3/envs/VLM_Poisoning/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Running i2i attack ######\n",
    "original_sizes_list = []\n",
    "X_adv_list = []\n",
    "all_losses = []\n",
    "loss_attack_list = []\n",
    "start_idx = 0\n",
    "end_idx = 0\n",
    "# for j, (image_target, target_cap) in  tqdm(enumerate(target_dataloader)):\n",
    "for j, (image_target, target_cap) in  tqdm(enumerate(target_dataloader), desc=\"Target Dataloader\", position=0, leave=True):\n",
    "    target_name = target_cap[\"name\"]\n",
    "    target_path = os.path.join(args[\"poison_save_pth\"], target_cap[\"name\"])\n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    ###### Resume by checking already saved images ######\n",
    "    saved_images = set(os.listdir(target_path))\n",
    "    saved_image_ids = {int(fname.split('.')[0]) for fname in saved_images if fname.endswith(('.png', '.jpg', '.jpeg'))}\n",
    "\n",
    "    # show_img(image_target, (\"Working on target:\", target_name))\n",
    "\n",
    "    for i, (image_base, base_cap) in  tqdm(enumerate(base_dataloader), desc=f\"Processing {target_name}\", total=len(base_dataloader), position=1, leave=False):\n",
    "        if base_cap['name'] in saved_image_ids:\n",
    "                print(f\"{base_cap['name']} already processed for {target_cap['name']}, skipping...\")\n",
    "                continue\n",
    "\n",
    "        print('image name = ', base_cap['name'])\n",
    "        image_base, image_target = image_base.to(device), image_target.to(device)\n",
    "\n",
    "        X_adv, loss_attack = i2i_EnsembleAttack(\n",
    "                model=encoder_model,\n",
    "                image=image_base,\n",
    "                target=image_target,\n",
    "                iters=args['iter_attack'],\n",
    "                lr=args['lr_attack'],\n",
    "                eps=args['eps']\n",
    "        )\n",
    "\n",
    "\n",
    "        ###### Save poisoned images after each batch ######\n",
    "        img_pth = os.path.join(target_path, f\"{base_cap['name']}.png\")\n",
    "        image_to_save = X_adv\n",
    "        print()\n",
    "        if base_cap['name'] not in saved_image_ids:  # Only save if it doesn't already exist\n",
    "            print(\"Max Pixel Difference between Adversarial Image and Base *255:\", torch.max(torch.abs(X_adv-image_base)).item()*255)\n",
    "            save_image(image_to_save.cpu(), img_pth)\n",
    "            print(f\"Saved poisoned image {base_cap['name']} to {img_pth}\") # mathvista\n",
    "\n",
    "        # show_img(((X_adv-image_base).to(torch.uint8).cpu()*10), (\"Adversarial Noise\", img_pth))\n",
    "        # show_img(image_to_save.cpu(), (\"Adversarial Image\", img_pth))\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Here to stop execution for this section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathvista Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacking_model=\"internvl\"\n",
    "captioning_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mxy/miniconda3/envs/VLM_Poisoning/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/mxy/miniconda3/envs/VLM_Poisoning/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2025-01-10 13:40:15,497] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "from poison_utils import *\n",
    "\n",
    "from eval_models import internlm, internvl\n",
    "from eval_models import gpt\n",
    "\n",
    "from build_query import create_query_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def verify_response(response):\n",
    "    if isinstance(response, str):\n",
    "        response = response.strip() \n",
    "    if response == \"\" or response == None:\n",
    "        return False\n",
    "    if \"Response Error\" in response:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def evaluate_code(code_string):\n",
    "    # execute_code_and_capture_output\n",
    "    # Backup the original stdout\n",
    "    old_stdout = sys.stdout\n",
    "    \n",
    "    # Redirect stdout to capture the output\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    \n",
    "    # Try executing the code and capture any exception\n",
    "    error = None\n",
    "    try:\n",
    "        exec(code_string)\n",
    "    except Exception as e:\n",
    "        error = e\n",
    "    \n",
    "    # Restore the original stdout\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    # Get the captured output\n",
    "    captured_output = new_stdout.getvalue()\n",
    "    if isinstance(captured_output, str):\n",
    "        captured_output = captured_output.strip()\n",
    "    \n",
    "    # Return the captured output or error\n",
    "    return captured_output, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"mini_MathVista_grid\"\n",
    "class Args:\n",
    "    def __init__(self, task_name, attacking_model, captioning_model):\n",
    "        # Input\n",
    "        self.poison_data_dir = f'data/poisons/{task_name}'\n",
    "        self.input_file = 'questions.json'\n",
    "        self.task_data_pth = f'data/{task_name}'\n",
    "\n",
    "        # Output\n",
    "        self.output_dir = f'results/{attacking_model}'\n",
    "        self.output_file = f'output_{attacking_model}.json'\n",
    "\n",
    "        # Model\n",
    "        self.model = captioning_model\n",
    "        self.key = \"\"\n",
    "\n",
    "        # Query\n",
    "        self.query_file = None\n",
    "        # self.caption_file = '../data/texts/captions_bard.json'\n",
    "        # self.ocr_file = '../data/texts/ocrs_easyocr.json'\n",
    "        self.shot_type = 'solution'\n",
    "        self.shot_num = 0\n",
    "        self.use_caption = False\n",
    "        self.use_ocr = False\n",
    "\n",
    "        # Other settings\n",
    "        self.rerun = False\n",
    "        self.debug = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Args({self.__dict__})\"\n",
    "\n",
    "args = Args(task_name, attacking_model, captioning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # load data\n",
    "    input_file = os.path.join(args.task_data_pth, args.input_file)\n",
    "    print(f\"Reading {input_file}...\")\n",
    "    data = read_json(input_file)\n",
    "    # load or create query data\n",
    "    if args.query_file:\n",
    "        query_file = os.path.join(args.task_data_pth, args.query_file)\n",
    "        if os.path.exists(query_file):\n",
    "            print(f\"Loading existing {query_file}...\")\n",
    "            query_data = read_json(query_file)\n",
    "    else:\n",
    "        print(\"\\nCreating new query...\")\n",
    "        # load caption\n",
    "        caption_data = {}\n",
    "        if args.use_caption:\n",
    "            caption_file = args.caption_file\n",
    "            if os.path.exists(caption_file):\n",
    "                print(f\"Reading {caption_file}...\")\n",
    "                try:\n",
    "                    caption_data = read_json(caption_file)[\"texts\"]\n",
    "                    print(\"Caption data loaded.\")\n",
    "                except:\n",
    "                    print(\"Caption data not found!! Please Check.\")                    \n",
    "        # load ocr\n",
    "        ocr_data = {}\n",
    "        if args.use_ocr:\n",
    "            ocr_file = args.ocr_file\n",
    "            if os.path.exists(ocr_file):\n",
    "                print(f\"Reading {ocr_file}...\")\n",
    "                try:\n",
    "                    ocr_data = read_json(ocr_file)[\"texts\"]\n",
    "                    print(\"OCR data loaded.\")\n",
    "                except:\n",
    "                    print(\"OCR data not found!! Please Check.\")\n",
    "        # create query\n",
    "        query_data = create_query_data(data, caption_data, ocr_data, args)\n",
    "\n",
    "    # output file\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(args.output_dir, args.output_file)\n",
    "    \n",
    "    # load results\n",
    "    if os.path.exists(output_file):\n",
    "        print(\"\\nResults already exist.\")\n",
    "        print(f\"Reading {output_file}...\")\n",
    "        results = read_json(output_file)\n",
    "    else:\n",
    "        results = {}\n",
    "\n",
    "    # load model\n",
    "    print(f\"\\nLoading {args.model}...\")\n",
    "    if args.model == 'bard':\n",
    "        if args.key == '':\n",
    "            print(\"Loading key from environment variable\")\n",
    "            key = os.environ['_BARD_API_KEY']\n",
    "        else:\n",
    "            key = args.key\n",
    "        model = bard.Bard_Model(key)\n",
    "    \n",
    "    elif \"gpt\" in args.model:\n",
    "        if args.key == '':\n",
    "            print(\"Loading token from environment variable\")\n",
    "            key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        else:\n",
    "            key = args.key\n",
    "        model = gpt.GPT_Model(args.model, key)\n",
    "    \n",
    "    elif \"claude\" in args.model:\n",
    "        if args.key == '':\n",
    "            print(\"Loading token from environment variable\")\n",
    "            key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        else:\n",
    "            key = args.key\n",
    "        model = claude.Claude_Model(args.model, key)\n",
    "    elif \"internlm\" in args.model:\n",
    "        model = internlm.InternLM_Model()\n",
    "    elif \"llava_one_v\" in args.model:\n",
    "        model = llava_one_v.Llava_One_V()\n",
    "    elif \"internvl\" in args.model:\n",
    "        model = internvl.InternVL_Model()\n",
    "            \n",
    "    \n",
    "    print(f\"Model loaded.\")\n",
    "    \n",
    "    # build final test pid list\n",
    "    test_pids = list(data.keys())\n",
    "    print(\"\\nNumber of test problems in total:\", len(test_pids))\n",
    "\n",
    "    available_directories = [d for d in os.listdir(args.poison_data_dir) if os.path.isdir(os.path.join(args.poison_data_dir, d))]\n",
    "    target_names = read_json(os.path.join(args.task_data_pth, \"target/caps.json\"))\n",
    "    if not all([name in available_directories for name in target_names]):\n",
    "        print(\"Not all targets have directories. Working with:\", available_directories)\n",
    "    target_names = [item[\"name\"] for item in target_names[\"annotations\"] if item[\"name\"] in available_directories]\n",
    "\n",
    "    skip_pids = []\n",
    "    if not args.rerun:\n",
    "        print(\"\\nRemoving problems with existing valid response...\")\n",
    "        for i, name in enumerate(target_names):\n",
    "            skip_pids.append([])\n",
    "            for pid in test_pids:\n",
    "                # print(f\"Checking {pid}...\")\n",
    "                if pid in results and 'response' in results[pid]:\n",
    "                    response = results[pid][name]['response']\n",
    "                    if verify_response(response):\n",
    "                        # print(f\"Valid response found for {pid}.\")\n",
    "                        skip_pids[i].append(pid)\n",
    "    else:\n",
    "        print(\"\\nRerun answer extraction for all problems...\")\n",
    "\n",
    "    test_pids = [[pid for pid in test_pids if pid not in target_skip_pids] for target_skip_pids in skip_pids]\n",
    "    print(\"Number of test problems to run for each target:\", {target_names[i]: len(target_pids) for i, target_pids in enumerate(test_pids)})\n",
    "    # print(test_pids)\n",
    "\n",
    "    \n",
    "\n",
    "    # tqdm, enumerate results\n",
    "    for i, target_name in enumerate(target_names):\n",
    "        for _, pid in enumerate(tqdm(test_pids[i])):\n",
    "            problem = data[pid]\n",
    "            query = query_data[pid]\n",
    "            image = problem['image']\n",
    "            image_path = os.path.join(args.poison_data_dir, target_name, image)\n",
    "\n",
    "            if args.debug:\n",
    "                print(\"--------------------------------------------------------------\")\n",
    "            print(f\"\\nGenerating response for {pid}...\")\n",
    "            try:\n",
    "                response = model.get_response(image_path, query)\n",
    "                new_caption = model.get_response(image_path, \"describe what is in this image\")\n",
    "                # print(f\"Response: {response}\")\n",
    "                if pid not in results:\n",
    "                    results[pid] = problem\n",
    "                if \"targets\" not in results[pid]:\n",
    "                    results[pid][\"targets\"] = {}\n",
    "                if target_name not in results[pid][\"targets\"]:\n",
    "                    results[pid][\"targets\"][target_name] = {}\n",
    "\n",
    "                results[pid][\"targets\"][target_name]['query'] = query\n",
    "                results[pid][\"targets\"][target_name]['model_description'] = new_caption\n",
    "                if args.shot_type == 'solution':\n",
    "                    results[pid][\"targets\"][target_name]['response'] = response\n",
    "                else:\n",
    "                    output, error = evaluate_code(response)\n",
    "                    results[pid][\"targets\"][target_name]['response'] = response\n",
    "                    results[pid][\"targets\"][target_name]['execution'] = output\n",
    "                    results[pid][\"targets\"][target_name]['error'] = str(error)\n",
    "                if args.debug:\n",
    "                    print(f\"\\n#Query: \\n{query}\")\n",
    "                    print(f\"\\n#Response: \\n{response}\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error in extracting answer for {pid}\")\n",
    "                results[pid][target_name]['error'] = e\n",
    "        \n",
    "            try:\n",
    "                print(f\"Saving results to {output_file}...\")\n",
    "                save_json(results, output_file)\n",
    "                print(f\"Results saved.\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error in saving {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from poison_utils import *\n",
    "\n",
    "# OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from prompts.ext_ans import demo_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_extraction(extraction):\n",
    "    extraction = extraction.strip()\n",
    "    if extraction == \"\" or extraction == None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def create_test_prompt(demo_prompt, query, response):\n",
    "    demo_prompt = demo_prompt.strip()\n",
    "    test_prompt = f\"{query}\\n\\n{response}\"\n",
    "    full_prompt = f\"{demo_prompt}\\n\\n{test_prompt}\\n\\nExtracted answer: \"\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "def extract_answer(response, problem, target_name, model=None, quick_extract=False):\n",
    "    question_type = problem['question_type']\n",
    "    answer_type = problem['answer_type']\n",
    "    choices = problem['choices']\n",
    "    query = problem[target_name]['query']\n",
    "    pid = problem['pid']\n",
    "\n",
    "    if response == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if question_type == 'multi_choice' and response in choices:\n",
    "        return response\n",
    "    \n",
    "    if answer_type == \"integer\":\n",
    "        try:\n",
    "            extraction = int(response)\n",
    "            return str(extraction)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if answer_type == \"float\":\n",
    "        try:\n",
    "            extraction = str(float(response))\n",
    "            return extraction\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # quick extraction\n",
    "    if quick_extract:\n",
    "        print(\"Quickly extracting answer...\")\n",
    "        # The answer is \"text\". -> \"text\"\n",
    "        try:\n",
    "            result = re.search(r'The answer is \"(.*)\"\\.', response)\n",
    "            if result:\n",
    "                extraction = result.group(1)\n",
    "                return extraction\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # general extraction\n",
    "    try:\n",
    "        full_prompt = create_test_prompt(demo_prompt, query, response)\n",
    "        # extraction = model.get_chat_response(full_prompt)\n",
    "        extraction = get_chat_response(full_prompt, openai.api_key)\n",
    "        return extraction\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error in extracting answer for {pid}\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"output_dir\": f\"results/{model}\",\n",
    "    \"output_file\": f\"output_{model}.json\",\n",
    "    \"response_label\": ,\n",
    "    \"llm_engine\": ,\n",
    "    \"number\": ,\n",
    "    \"quick_extract\": ,\n",
    "    \"rerun\": , \n",
    "    \"save_every\": ,\n",
    "    \"output_label\": \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "label = args.response_label\n",
    "result_file = os.path.join(args.output_dir, args.output_file)\n",
    "\n",
    "if args.output_label != '':\n",
    "    output_file = result_file.replace('.json', f'_{args.output_label}.json')\n",
    "else:\n",
    "    output_file = result_file\n",
    "\n",
    "# read results\n",
    "print(f\"Reading {result_file}...\")\n",
    "results = read_json(result_file)\n",
    "\n",
    "# model = llama_3_1.LLaMA()\n",
    "\n",
    "# full pids\n",
    "full_pids = list(results.keys())\n",
    "if args.number > 0:\n",
    "    full_pids = full_pids[:min(args.number, len(full_pids))]\n",
    "print(\"Number of testing problems:\", len(full_pids))\n",
    "\n",
    "# test pids\n",
    "if args.rerun:\n",
    "    test_pids = full_pids\n",
    "else:\n",
    "    test_pids = []\n",
    "    for pid in full_pids:\n",
    "        # print(pid)\n",
    "        if 'extraction' not in results[pid] or not verify_extraction(results[pid]['extraction']):\n",
    "            test_pids.append(pid)\n",
    "\n",
    "\n",
    "test_num = len(test_pids)\n",
    "print(\"Number of problems to run:\", test_num)\n",
    "# print(test_pids)\n",
    "\n",
    "# tqdm, enumerate results\n",
    "for i, pid in enumerate(tqdm(test_pids)):\n",
    "    target_names = results[pid][\"targets\"].keys()\n",
    "    for name in target_names:\n",
    "        problem = results[pid]\n",
    "\n",
    "        assert label in problem[\"targets\"][name]\n",
    "        response = problem[\"targets\"][name][label]       \n",
    "\n",
    "        \n",
    "        # extraction  = extract_answer(response, problem, name, model, args.quick_extract)\n",
    "        extraction  = extract_answer(response, problem, name, None, args.quick_extract)\n",
    "        results[pid][\"targets\"][name]['extraction'] = extraction\n",
    "\n",
    "        if i % args.save_every == 0 or i == test_num - 1:\n",
    "            print(f\"Saving results to {output_file}...\")\n",
    "            save_json(results, output_file)\n",
    "            print(f\"Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLM_Poisoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
